<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mikezhang95.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mikezhang95.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-02T14:48:45+00:00</updated><id>https://mikezhang95.github.io/feed.xml</id><title type="html">Yuan Zhang</title><subtitle> This is the personal webpage of Yuan. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Model-based Reinforcement Learning</title><link href="https://mikezhang95.github.io/blog/2026/01/28/model_based-rl/" rel="alternate" type="text/html" title="Model-based Reinforcement Learning"/><published>2026-01-28T00:00:00+00:00</published><updated>2026-01-28T00:00:00+00:00</updated><id>https://mikezhang95.github.io/blog/2026/01/28/model_based-rl</id><content type="html" xml:base="https://mikezhang95.github.io/blog/2026/01/28/model_based-rl/"><![CDATA[<p>Model-based reinforcement learning (MBRL) aims to improve sample efficiency by explicitly learning a model of the environment dynamics and using it for planning, policy optimization, or both. Compared to model-free RL, MBRL introduces inductive bias and structure, at the cost of model bias and potential instability. This post surveys modern MBRL methods with an emphasis on <em>practical algorithmic designs</em> and <em>optimization choices</em> that make them work in real systems.</p> <p>[Papers: World Models, Muzero, EfficientZero, PETS, MBPO, TDMPC, Dreamer]</p> <hr/> <h2 id="1-motivation-why-model-based-rl">1. Motivation: Why Model-Based RL?</h2> <p>In many real-world domains (robotics, autonomous driving, manipulation), environment interactions are expensive. MBRL attempts to answer:</p> <blockquote> <p>Can we learn a sufficiently accurate world model and exploit it to reduce real-world samples?</p> </blockquote> <p>Benefits:</p> <ul> <li>Higher data efficiency</li> <li>Explicit dynamics modeling</li> <li>Natural integration with classical control</li> </ul> <p>Challenges:</p> <ul> <li>Compounding model errors</li> <li>Distribution shift when policies exploit model inaccuracies</li> <li>Optimization instability when planning over learned dynamics</li> </ul> <hr/> <h2 id="2-taxonomy-of-model-based-rl">2. Taxonomy of Model-Based RL</h2> <h3 id="21-planning-with-learned-models">2.1 Planning with Learned Models</h3> <p>Given Learn a dynamics model: $s_{t+1} = f_\theta(s_t, a_t)$</p> <p>Then use <strong>online planning</strong> (e.g., Model Predictive Control) to select actions:</p> <ul> <li>Shooting methods</li> <li>Cross-Entropy Method (CEM)</li> <li>Random sampling with trajectory scoring</li> </ul> <p>Representative methods:</p> <ul> <li>PILCO</li> <li>PETS (Probabilistic Ensembles with Trajectory Sampling)</li> </ul> <p>Key idea: <em>do not train a policy directly; re-plan at every step.</em></p> <hr/> <h3 id="22-model-based-policy-optimization">2.2 Model-Based Policy Optimization</h3> <p>Instead of pure planning, the learned model is used to <strong>generate synthetic data</strong> for training a policy.</p> <p>Typical loop:</p> <ol> <li>Collect real transitions</li> <li>Train a dynamics model</li> <li>Roll out the policy in the model (short horizon)</li> <li>Update the policy using real + imagined data</li> </ol> <p>Representative methods:</p> <ul> <li>MBPO</li> <li>STEVE</li> </ul> <p>Key tradeoff: rollout horizon vs. model bias.</p> <hr/> <h3 id="23-latent-world-models">2.3 Latent World Models</h3> <p>High-dimensional observations (images) are difficult to model directly.</p> <p>Latent world models:</p> <ul> <li>Learn an encoder ( z_t = e(s_t) )</li> <li>Learn latent dynamics ( z_{t+1} = g(z_t, a_t) )</li> <li>Train policy entirely in latent space</li> </ul> <p>Representative methods:</p> <ul> <li>World Models</li> <li>Dreamer / DreamerV2 / DreamerV3</li> </ul> <p>Advantages:</p> <ul> <li>Compact representations</li> <li>Improved generalization</li> <li>Scales to vision-based RL</li> </ul> <hr/> <h2 id="3-handling-model-uncertainty">3. Handling Model Uncertainty</h2> <h3 id="31-ensemble-models">3.1 Ensemble Models</h3> <p>Train multiple models ( {f_{\theta_i}} ) and:</p> <ul> <li>Sample a model per rollout</li> <li>Penalize uncertainty during planning</li> <li>Avoid overconfident predictions</li> </ul> <p>Ensembles are one of the most effective and widely used tricks in practical MBRL.</p> <hr/> <h3 id="32-short-horizon-rollouts">3.2 Short-Horizon Rollouts</h3> <p>Long rollouts amplify small errors: [ \epsilon_{t+k} \approx O(\epsilon^k) ]</p> <p>MBPO shows that <strong>many short rollouts</strong> outperform <strong>few long rollouts</strong>.</p> <hr/> <h2 id="4-optimization-and-planning">4. Optimization and Planning</h2> <p>Common planners:</p> <ul> <li>Random shooting</li> <li>CEM (iteratively refines action distributions)</li> <li>Gradient-based optimization (less stable)</li> </ul> <p>Hybrid designs:</p> <ul> <li>Learned value function + MPC</li> <li>Learned policy as proposal distribution for planning</li> </ul> <hr/> <h2 id="quick-summary">Quick Summary</h2> <table> <thead> <tr> <th>Issue</th> <th>Mitigation</th> </tr> </thead> <tbody> <tr> <td>Compounding error</td> <td>Short rollouts, ensembles</td> </tr> <tr> <td>Model exploitation</td> <td>Conservative policy updates</td> </tr> <tr> <td>High-dimensional inputs</td> <td>Latent dynamics</td> </tr> <tr> <td>Training instability</td> <td>Regularization, replay mixing</td> </tr> </tbody> </table> <hr/> <h2 id="references">References</h2> <ul> <li>Chua et al., <em>Deep RL in a Handful of Trials using Probabilistic Dynamics Models</em></li> <li>Janner et al., <em>When to Trust Your Model: MBPO</em></li> <li>Hafner et al., <em>Dreamer: Learning Behaviors from Pixels</em></li> </ul> <hr/> <h2 id="changelog">Changelog</h2> <p>2026-01-28: create the page</p>]]></content><author><name>Yuan Zhang</name></author><category term="Blog"/><category term="RL"/><summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) aims to improve sample efficiency by explicitly learning a model of the environment dynamics and using it for planning, policy optimization, or both. Compared to model-free RL, MBRL introduces inductive bias and structure, at the cost of model bias and potential instability. This post surveys modern MBRL methods with an emphasis on practical algorithmic designs and optimization choices that make them work in real systems.]]></summary></entry><entry><title type="html">Generative Models</title><link href="https://mikezhang95.github.io/blog/2026/01/27/generative_models/" rel="alternate" type="text/html" title="Generative Models"/><published>2026-01-27T00:00:00+00:00</published><updated>2026-01-27T00:00:00+00:00</updated><id>https://mikezhang95.github.io/blog/2026/01/27/generative_models</id><content type="html" xml:base="https://mikezhang95.github.io/blog/2026/01/27/generative_models/"><![CDATA[<p>Generative modeling is a core building block across modern machine learning, from image synthesis to trajectory generation and world modeling.</p> <p>This post reviews two dominant paradigms: generative adversarial networks (GANs) and diffusion models, and their variants.</p> <hr/> <h2 id="1-what-is-a-generative-model">1. What Is a Generative Model?</h2> <p>Given data \(x \sim p_\text{data}(x)\), a generative model learns an approximation \(p_\theta(x)\) such that:</p> <ul> <li>generates samples that resemble real data</li> <li>covers the full data distribution.</li> </ul> <p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="generative-overview" style="zoom: 50%;"/></p> <hr/> <h2 id="2-todo-variational-auto-encoder-vae">2. TODO: Variational Auto-Encoder (VAE)</h2> <hr/> <h2 id="3-generative-adversarial-networks-gans-1">3. Generative Adversarial Networks (GANs) [1]</h2> <h3 id="31-core-idea">3.1 Core Idea</h3> <p>GANs train two networks:</p> <ul> <li>A generator \(G\) generates synthetic samples (\(x = G(z)\)), given a noise variable input \(z\) who usually follows a standard normal distribution and introducespotential output diversity. It is trained to trick the discriminator to offer a high probability.</li> <li>Discriminator \(D\) outputs the <strong>probability</strong> of a given sample coming from the real dataset (\(D(x)\)). It is trained to distinguish the fake samples from the real ones.</li> </ul> <p><img src="https://lilianweng.github.io/posts/2017-08-20-gan/GAN.png" alt="GAN" style="zoom:50%;"/></p> <p>These two models compete against each other during the training process: the generator \(G\) is trying hard to trick the discriminator, while the critic model \(D\) is trying hard not to be cheated, which forms a zero-sum game.</p> <p>Suppose \(p_r,p_g,p_z\) distributions over the real sample \(x\), generated sample \(x\) and random noise \(z\) (usually a standard normal distribution). The minimax objective is like:</p> \[\begin{aligned} \min_G \max_D L(G, D) = \mathbb{E}_{x \sim p_r}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))] \\ = \mathbb{E}_{x \sim p_r}[\log D(x)] + \mathbb{E}_{x \sim p_g}[\log(1 - D(x)]. \end{aligned}\] <h3 id="32-what-is-the-optimal-value-for-d">3.2 What is the optimal value for \(D\)?</h3> <p>Examinte the best value for \(D\) by calulating the stationary point:</p> \[D^*(x) = \frac{p_r(x)}{p_r(x) + p_g(x)}.\] <h3 id="33-what-does-the-loss-function-represent">3.3 What does the loss function represent?</h3> <p>Given the optimal discriminator \(D^*\), find the relations between real and synthetic data distributions \(p_r\) and \(p_g\):</p> \[\begin{aligned} L(G, D^*) = \mathbb{E}_{x \sim p_r}[\log \frac{p_r(x)}{p_r(x)+p_g(x)}] + \mathbb{E}_{x \sim p_g}[\log \frac{p_g(x)}{p_r(x)+p_g(x)}] \\ = D_{KL}(p_r\|\frac{p_r+p_g}{2}) - \log2 + D_{KL}(p_g\|\frac{p_r+p_g}{2}) - \log2 \\ = 2D_{JS}(p_r \| p_g) - \log4. \end{aligned}\] <h3 id="34-todo-problems-in-gans">3.4 TODO: Problems in GANs</h3> <ul> <li>Hard to achieve Nash equilibrium</li> <li>Low dimensional supports</li> <li>Vanishing gradient</li> <li>Mode collapse</li> <li>Lack of a proper evaluation metric (unknown likelihood of \(p_g\))</li> </ul> <h3 id="35-wasserstein-gan-wgan-2">3.5 Wasserstein GAN (WGAN) [2]</h3> <p><a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein Distance</a> is a measure of the distance between two probability distributions, which can be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution. When dealing with the continuous probability domain, the distance formula becomes:</p> \[W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x_1,x_2) \sim \gamma}[ \|x_1 - x_2\| ], \\\\\] <p>where \(\tau(x_1,x_2)\) states the joint distribution that satisfies the boundary conditions of \(\int_{x_2} \gamma(x_1,x_2)dx_2 = p_r(x_1)\) and \(\int_{x_1} \gamma(x_1,x_2)dx_1 = p_g(x_2)\).</p> <h4 id="todo-why-wasserstein-is-better-than-js-or-kl-divergence">TODO: Why Wasserstein is better than JS or KL divergence?</h4> <p><img src="https://lilianweng.github.io/posts/2017-08-20-gan/wasserstein_simple_example.png" alt="wasserstein_simple_example" style="zoom:50%;"/></p> <h4 id="use-wasserstein-distance-as-gan-loss-function">Use Wasserstein distance as GAN loss function</h4> <p>It is intractable to exhaust all the possible joint distributions in \(\Pi(p_r, p_g)\) to compute \(\inf_{\gamma\sim \Pi(p_r, p_g)}\) . Thus the authors proposed a smart transformation of the formula based on the <strong>Kantorovich-Rubinstein duality</strong> to:</p> \[D_W(p_r, p_g) = \frac{1}{K} \sup_{\|f\| \le K} \mathbb{E}_{x\sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g} [f(x)],\] <p>where the real-valued function \(f: \mathbb{R} \to \mathbb{R}\) should be K-Lipschitz continuous. Read this <a href="https://vincentherrmann.github.io/blog/wasserstein/">blog</a> for more about this duality transformation. In the modified Wasserstein GAN, the “discriminator” model is used to find a <strong>good</strong> function \(f_w\) parametrized by \(w \in W\) and the loss function is configured as measuring the Wasserstein distance between \(p_r\) and \(p_g\):</p> \[L(p_r,p_g) = D_W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x\sim p_r}[f_w(x)] - \mathbb{E}_{x \sim p_g} [f_w(x)].\] <p>To maintain the K-Lipschitz continuity of function \(f_w\) during training, the paper presents a simple but very practical trick: After every gradient update, clamp the weights \(w\) to a small window, such as \([-0.01, 0.01]\).</p> <h3 id="36-connections-to-actor-critic-methods-3">3.6 Connections to Actor-Critic Methods [3]</h3> <h4 id="the-mathematical-bridge-bilevel-optimization">The Mathematical Bridge: Bilevel Optimization</h4> \[\begin{aligned} x^* = \arg \min_x F(x, y^*(x)) \quad\text{(Outer Opt.)} \\ y^*(x) = \arg \min_y f(x, y) \quad\text{(Inner Opt.)}. \end{aligned}\] <table> <thead> <tr> <th style="text-align: left">Component</th> <th style="text-align: left">GANs</th> <th style="text-align: left">Actor-Critic Methods</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Outer Model (x)</strong></td> <td style="text-align: left">Discriminator (\(D\))</td> <td style="text-align: left">Critic (\(Q\)-function)</td> </tr> <tr> <td style="text-align: left"><strong>Outer Objective (F)</strong></td> <td style="text-align: left">\(-L(G, D)\)</td> <td style="text-align: left">\(-\mathbb{E}_{s,a}[D_{KL}(\mathbb{E}_{r,s',a'}[r+\gamma Q(s', a')]|Q (s, a)]\)</td> </tr> <tr> <td style="text-align: left"><strong>Inner Model (y)</strong></td> <td style="text-align: left">Generator (\(G\))</td> <td style="text-align: left">Actor (Policy \(\pi\))</td> </tr> <tr> <td style="text-align: left"><strong>Inner Objective (f)</strong></td> <td style="text-align: left">\(-\mathbb{E}_{z\sim p_z}[\log D(G(z))]\)</td> <td style="text-align: left">\(-\mathbb{E}_{s \sim \mu,a\sim\pi}[Q(s, a)]\)</td> </tr> </tbody> </table> <p>​</p> <h4 id="gans-as-a-kind-of-actor-critic">GANs as a kind of Actor-Critic</h4> <p>For an RL environment with:</p> <ul> <li>State: stateless</li> <li>Action: generates an entire image</li> <li>Environment step: randomly decides to show either a real image of a generated image</li> <li>Reward: +1 if a real image is shown, 0 if the generated image is shown</li> </ul> <p>reward/state does not depend on action.</p> <hr/> <h2 id="4-diffusion-models">4. Diffusion Models</h2> <p>Several diffusion-based generative models have been proposed with similar ideas underneath, including <em>diffusion probabilistic models</em> (<strong>DPM</strong>; <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>), <em>noise-conditioned score network</em> (<strong>NCSN</strong>; <a href="https://arxiv.org/abs/1907.05600">Yang &amp; Ermon, 2019</a>), and <em>denoising diffusion probabilistic models</em> (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239">Ho et al. 2020</a>).</p> <h3 id="41-forward-diffusion-process-from-data-to-noise">4.1 Forward Diffusion Process: From Data to Noise</h3> <p>Given a data point sampled from a real data distribution \(x_0 \sim p_r(x)\), a forward diffusion process, we can gradually add small amount of Gaussian noise in \(T\) steps, producing a sequence os noisy samples \(x_1,\dots,x_T\). The step sizes are controlled by a variance schedule \({\beta_t \in (0,1)}_{t=1}^T\):</p> \[q(x_t|x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I}), q(x_{0:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}).\] <p>When \(T\to \infty\), \(x_T\) is equivalent to an isotropic Gaussian distribution.</p> <h4 id="reparametrization-tricks-from-x_0-to-x_t">Reparametrization tricks: from \(x_0\) to \(x_t\)</h4> \[\begin{aligned} x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t} \epsilon_{t-1} \quad\text{;where $\epsilon_{t-1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$} \\ = \sqrt{(1-\beta_t)(1-\beta_{t-1})}x_{t-2} + \sqrt{1-(1-\beta_t)(1-\beta_{t-1})} \bar{\epsilon}_{t-2} \quad\text{;where $\bar{\epsilon}_{t-2}$ merges two Gaussians} \\ = \dots \\ = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon \quad\text{;where $\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$}\\ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)\mathbf{I}). \end{aligned}\] <h4 id="connections-with-stochasitic-gradient-langevin-dynamics">Connections with stochasitic gradient Langevin dynamics</h4> <p>Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, <em>stochastic gradient Langevin dynamics</em> (<strong>SGLD</strong>; <a href="https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf">Welling &amp; Teh 2011</a>) can produce samples from a probability density \(p(x)\) using only the gradients \(\nabla_x \log p(x)\) in a Markov chain of updates:</p> \[x_t = x_{t-1} + \Delta_t/2 \nabla_x \log p(x_{t-1}) + \sqrt{\Delta_t}\epsilon_t, \quad\text{where $\epsilon_{t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$},\] <p>where \(\Delta_t\) is the step size. When \(T \to \infty, \epsilon \to 0\), $x_T$ equals to the true probability \(p(x)\). The Gaussian noise avoids collapses into local minima.</p> <h3 id="42-reverse-diffusion-process-from-noise-to-data">4.2 Reverse Diffusion Process: From Noise to Data</h3> <p>If we can reverse the above process and sample from \(p_{\theta}(x_{t-1}\vert x_t)\), we will be able to recreate the true sample from a Gaussian noise input, \(x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\):</p> \[p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};\mu_{\theta}(x_t, t), \Sigma_\theta(x_t, t)), p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t).\] <h4 id="distribution-representation-1-from-x_t-x_0-to-x_t-1">Distribution Representation 1: from \(x_t, x_0\) to \(x_{t-1}\)</h4> <p>It is noteworthy that the reverse conditional probability is tractable when conditioned on real sample \(x_0\) :</p> \[\begin{aligned} q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1};\tilde{\mu}(x_t, x_0), \tilde{\beta}_t\mathbf{I}), \\ \tilde{\mu}(x_t, x_0) = \frac{\sqrt{1-\beta_t}( 1- \bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t} x_0 \\ \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t. \end{aligned}\] <h4 id="distribution-representation-2-from-x_t-to-x_t-1">Distribution Representation 2: from \(x_t\) to \(x_{t-1}\)</h4> <p>From the reparametrization tricks above, we can replace \(x_0\) with \(x_t\):</p> \[\begin{aligned} \tilde{\mu}(x_t) = \frac{\sqrt{1-\beta_t}( 1- \bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\frac{1}{\sqrt{\bar{\alpha}_t}} (x_t - \sqrt{1-\bar{\alpha}_t} \epsilon \\ = \frac{1}{\sqrt{1-\beta_t}} (x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon) \quad\text{;where $\bar{\alpha}_{t} = \bar{\alpha}_{t-1}(1-\beta_t)$} \end{aligned}\] <p>The above derivations can be achieved by Bayesian rule, see this <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">blog</a> for details. We want to use a parametrized function \(\mu_{\theta}(x_t, t)\) to represent \(\tilde{\mu}(x_t)\). Since \(x_t\) is known during training, we only need to learn to predict noises \(\epsilon\) with \(\epsilon_\theta(x_t, t)\).</p> <h3 id="43-training-and-sampling-process">4.3 Training and Sampling Process</h3> <p>TODO: dervie VLB loss:</p> <p>TODO: dervie NCSN loss:</p> <p>TODO: dervie DDPM loss:</p> \[\begin{aligned} L_\text{DDPM} = \mathbb{E}_{x_0, t \sim [1, T], \epsilon}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2 ] \\ = \mathbb{E}_{x_0, t \sim [1, T], \epsilon}[\|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\|^2 ] \\ \end{aligned}\] <p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM-algo.png" alt="DDPM-algo" style="zoom: 33%;"/></p> <h2 id="5-flow-matching-models">5. Flow-Matching Models</h2> <h3 id="51-mathematical-background">5.1 Mathematical Background</h3> <h4 id="forward-diffusion-as-stochasitic-diffential-equation-sde">Forward Diffusion as Stochasitic Diffential Equation (SDE)</h4> <p>If we make the diffusion step \(\Delta t \to 0\) and the forward diffusion process becomes a stochastic differential equation (SDE):</p> \[dx = f(x,t)dt + g(t)dw\] <p>where:</p> <ul> <li>\(w\) represents a Wiener process (i.e., Brownian random motion);</li> <li>\(f(\cdot, t)\) is called the <strong>drift;</strong></li> <li>\(g(t)\) is called the <strong>diffusion</strong> term.</li> </ul> <table> <thead> <tr> <th>Diffusion Methods</th> <th>\(f(x, t)\)</th> <th>\(g(t)\)</th> </tr> </thead> <tbody> <tr> <td>DDPM</td> <td>\(-\frac{1}{2}\beta_tx\)</td> <td>\(\sqrt{\beta_t}\)</td> </tr> <tr> <td>NCSN</td> <td>\(0\)</td> <td>\(\sqrt{\frac{d\sigma_t^2}{dt}}\)</td> </tr> </tbody> </table> <p>By carefully designing noise scheduling \(\beta_t\) and \(\sigma_t\) , DDPM and NCSN can generate the same probability path \(\{p_t(x)\}\) and different SDEs are just different mathematical formulas to describe the same process,</p> <h4 id="equivelance-between-sde-and-ordinary-differential-equation-ode">Equivelance between SDE and Ordinary Differential Equation (ODE)</h4> <p><a href="https://arxiv.org/pdf/2011.13456">Song 2021</a> has proved that <em>for any SDE, it has a corresponding ordinary differential equation (ODE) inducing the same \(p_t(x)\)</em>, which is called <strong>probability flow ODE</strong> with the following format: \(\frac{dx}{dt} = f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x)\)</p> <p>This connection transforms a random “walk” to a deterministic “flow” along the ODE trajectory, (1) easing the sampling process of the diffusion process (<a href="https://arxiv.org/abs/2010.02502">DDIM, Song 2021</a>) and (2) inducing following simpler flow-matching methods.</p> <h3 id="52-flow-matching-methods">5.2 Flow-Matching Methods</h3> <p>Probability flow ODE relies on a global score-based function \(\nabla_x \log p_t(x)\) to generate samples, which is difficult to learn. While flow-matching methods directly design a simple path from real data \(x_0\) to pure noise \(x_1\). The simplest path is a constant-speed motion \(x_t=(1-t)x_0 + tx_1\). We need to learn a parametrized function \(v_{\theta}(x_t, t)\) to match the real speed $x_1-x_0$.</p> <h3 id="53-training-and-sampling-process">5.3 Training and Sampling Process</h3> <p><img src="https://raw.githubusercontent.com/mikezhang95/MyImages/main/img/202602021613294.png" alt="Screenshot 2026-02-02 at 16.13.04" style="zoom:80%;"/></p> <p>\(z\) and \(x\) can be the same space or \(z\) be the latent space and flow-matching happens on that space.</p> <h2 id="references">References</h2> <p>[1] Goodfellow, Ian, et al. <a href="https://arxiv.org/pdf/1406.2661.pdf">“Generative Adversarial Nets.”</a> Neurips, 2014.</p> <p>[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. <a href="https://arxiv.org/pdf/1701.07875.pdf">“Wasserstein GAN.”</a> ICML, 2017.</p> <p>[3] David Pfau, Oriol Vinyals. <a href="">“Connecting Generative Adversarial Networks and Actor-Critic Methods.”</a> arXiv, 2016.</p> <p>[4] Calvin Luo. <a href="https://arxiv.org/pdf/2208.11970">“Understanding Diffusion Models: A Unified Perspective.”</a> arXiv, 2022.</p> <p>TODO: unify references and hyperlinks in the blog.</p>]]></content><author><name>Yuan</name></author><category term="Blog"/><summary type="html"><![CDATA[Generative modeling is a core building block across modern machine learning, from image synthesis to trajectory generation and world modeling.]]></summary></entry><entry><title type="html">Inverse Reinforcement Learning</title><link href="https://mikezhang95.github.io/blog/2026/01/26/inverse_rl/" rel="alternate" type="text/html" title="Inverse Reinforcement Learning"/><published>2026-01-26T00:00:00+00:00</published><updated>2026-01-26T00:00:00+00:00</updated><id>https://mikezhang95.github.io/blog/2026/01/26/inverse_rl</id><content type="html" xml:base="https://mikezhang95.github.io/blog/2026/01/26/inverse_rl/"><![CDATA[<blockquote> <table> <tbody> <tr> <td>Date: 2026-01-30</td> <td>Estimated Reading Time: 30 min</td> <td>Author: Yuan Zhang</td> </tr> </tbody> </table> </blockquote> <p>Imitation Learning (IL) and Inverse Reinforcement Learning (IRL) address the problem of learning behavior from expert demonstrations, especially when defining an explicit reward function is difficult.</p> <p>[Papers: HLFH, GAIL]</p> <hr/> <h2 id="1-problem-setup">1. Problem Setup</h2> <p>Given expert trajectories: [ \tau_E = {(s_t, a_t)} ]</p> <p>Goal:</p> <ul> <li> <table> <tbody> <tr> <td>Learn a policy ( \pi(a</td> <td>s) ) (IL)</td> </tr> </tbody> </table> </li> <li>Or infer a reward function ( r(s,a) ) (IRL)</li> </ul> <hr/> <h2 id="2-behavioral-cloning-bc">2. Behavioral Cloning (BC)</h2> <p>BC treats imitation as supervised learning: [ \min_\theta \mathbb{E}<em>{(s,a)\sim \tau_E} [| \pi</em>\theta(s) - a |^2] ]</p> <h3 id="pros">Pros</h3> <ul> <li>Simple</li> <li>Stable</li> </ul> <h3 id="cons">Cons</h3> <ul> <li>Covariate shift</li> <li>Error accumulation</li> </ul> <hr/> <h2 id="3-inverse-reinforcement-learning">3. Inverse Reinforcement Learning</h2> <p>IRL assumes:</p> <blockquote> <p>The expert is (near-)optimal under an unknown reward.</p> </blockquote> <p>Classic IRL:</p> <ul> <li>Maximum entropy IRL</li> <li>Feature matching</li> </ul> <p>Main issue:</p> <ul> <li>Reward ambiguity (reward shaping equivalence)</li> </ul> <hr/> <h2 id="4-adversarial-imitation-learning">4. Adversarial Imitation Learning</h2> <h3 id="41-gail">4.1 GAIL</h3> <p>GAIL learns a discriminator: [ D(s,a) ]</p> <p>The policy is trained to fool the discriminator, similar to GANs.</p> <p>Interpretation:</p> <ul> <li>Implicit reward learning</li> <li>Avoids explicit reward engineering</li> </ul> <hr/> <h3 id="42-airl">4.2 AIRL</h3> <p>AIRL introduces a structured reward: [ r(s,a) = f_\theta(s,a) + \gamma h(s’) - h(s) ]</p> <p>Benefits:</p> <ul> <li>Reward transferability</li> <li>Better interpretability</li> </ul> <hr/> <h2 id="5-practical-training-strategies">5. Practical Training Strategies</h2> <ul> <li>BC warm-start + GAIL fine-tuning</li> <li>Off-policy adversarial IL</li> <li>Hybrid IL + RL pipelines</li> </ul> <hr/> <h2 id="6-when-to-use-irl">6. When to Use IRL?</h2> <p>Good fit:</p> <ul> <li>Reward is ambiguous</li> <li>Transfer across environments is required</li> </ul> <p>Poor fit:</p> <ul> <li>Dense, well-defined rewards</li> <li>Limited expert data</li> </ul> <hr/> <h2 id="7-key-references">7. Key References</h2> <ul> <li>Ho &amp; Ermon, <em>GAIL</em></li> <li>Fu et al., <em>AIRL</em></li> <li>Ziebart, <em>Maximum Entropy IRL</em></li> </ul> <hr/> <h2 id="8-open-problems">8. Open Problems</h2> <ul> <li>Sample-efficient IL</li> <li>Multi-agent imitation</li> <li>Foundation models for IL</li> </ul>]]></content><author><name>Yuan Zhang</name></author><category term="Blog"/><summary type="html"><![CDATA[Date: 2026-01-30 Estimated Reading Time: 30 min Author: Yuan Zhang]]></summary></entry><entry><title type="html">Autonomous Driving Algorithms</title><link href="https://mikezhang95.github.io/blog/2026/01/25/autonomous-driving/" rel="alternate" type="text/html" title="Autonomous Driving Algorithms"/><published>2026-01-25T00:00:00+00:00</published><updated>2026-01-25T00:00:00+00:00</updated><id>https://mikezhang95.github.io/blog/2026/01/25/autonomous-driving</id><content type="html" xml:base="https://mikezhang95.github.io/blog/2026/01/25/autonomous-driving/"><![CDATA[<blockquote> <table> <tbody> <tr> <td>Date: 2026-01-31</td> <td>Estimated Reading Time: 30 min</td> <td>Author: Yuan Zhang</td> </tr> </tbody> </table> </blockquote> <p>Autonomous driving systems combine learning-based prediction with classical estimation and optimization. This post surveys key components commonly found in modern driving stacks.</p> <p>[Multipath++, 3DGS, Kalman filter, iLQR]</p> <hr/> <h2 id="1-overview">1 Overview</h2> <p>state \(x_t\), control \(u_t\), observations \(z_t\)</p> <hr/> <h2 id="2-kalman-filter-state-estimation">2 Kalman Filter: State Estimation</h2> <p>The Kalman filter provides an optimal (minimum mean square error) recursive Bayesian estimate \(x_t\) for the state of a dynamic system under linear Gaussian assumptions.</p> <h3 id="21-linear-kalman-filter">2.1 Linear Kalman Filter</h3> <h4 id="state-model">State Model</h4> <p>The prediction model predicts the next state based on a linear dynamics model: \(x_{t+1} = F_t x_t + B_t u_t + w_t, w_t \sim \mathcal{N}(0, Q_t).\) The measurement model generates the observations with a linear transformation: \(z_t = H_t x_t + v_t, v_t \sim \mathcal{N}(0, R_t).\)</p> <h4 id="predict-update-cycle">Predict-Update Cycle</h4> <p>The filter maintains a posterior Gaussian estimate of the current state: \(p(x_t):=\mathcal{N}(x_t;\mu_t, \Sigma_t)\). The posterior comes from consists of 2 steps: the prediction step and update step.</p> <p>The prediction step calculates a prior from the prediction model:</p> \[\begin{aligned} \mu_{t|t-1} = F_t \mu_{t-1} + B_t u_t \\ \Sigma_{t|t-1} = F_t \Sigma_{t-1}F_t^T + Q_t \\ \end{aligned}\] <p>The update step updates this prior given new observations:</p> \[\begin{aligned} K_t = \Sigma_{t|t-1} H_t^T(H_y\Sigma_{t|t-1} H_t^T + R_t)^{-1} \\ \mu_t = \mu_{t|t-1} + K_t (z_t - H_t \mu_{t|t-1}) \\ \Sigma_{t} = (I - K_tH_t)\Sigma_{t|t-1} \\ \end{aligned}\] <p>Kalman filter is just an application of the combinations of multiple Gaussian distributions.</p> <hr/> <h3 id="22-extensions">2.2 Extensions</h3> <ul> <li>EKF: nonlinear dynamics</li> <li>UKF: sigma-point approximation</li> <li>Smoothing for offline estimation</li> </ul> <hr/> <h2 id="3-ilqr-iterative-lqr-trajectory-optimization">3 iLQR (Iterative LQR): Trajectory Optimization</h2> <p>iLQR solves discrete-time nonlinear optimal control problems (find optimal \(u^*_t\)) by iterative linearization and quadratic approximation.</p> <h3 id="31-optimal-control-problem">3.1 Optimal Control Problem</h3> \[\begin{aligned} \min_{u_0, \dot, u_{T-1}} c_T(x_T) + \sum_{t=0}^{T-1} c_t(x_t, u_u) \\ s.t. x_{t+1} = f(x_t, u_t), t=0, \dots, T-1 \end{aligned}\] <h3 id="32-ilqr">3.2 iLQR</h3> <p>At each iteration, linearize both transition function \(f\) and cost function \(c\) around the current trajectory \((\bar{x}_t, \bar{u}_t)\) to achieve an LQR problem: \(\begin{aligned} \min_{u_0, \dot, u_{T-1}} c_T(x_T) + \sum_{t=0}^{T-1} \delta x_t^T Q_{t} \delta x_t + q^T_{t} \delta x_t \\ s.t. \delta x_{t+1} \approx A_t \delta x_t + B_t \delta u_t, , t=0, \dots, T-1 \end{aligned}\)</p> <p>where \(\delta x_t = x_t - \bar{x}_t, \delta u_t = u_t - \bar{u}_t, A_t = \nabla_xf\|_{\bar{x}_t, \bar{u}_t}, B_t = \nabla_uf\|_{\bar{x}_t, \bar{u}_t}, q_t = \nabla_xc_t\|_{\bar{x}_t, \bar{u}_t}, Q_t = \nabla_{xx}c_t\|_{\bar{x}_t, \bar{u}_t}\), etc. We can extend the cost term on \(u_t\) as quadratic and linear terms of \(R_t\) and \(r_t\) as well.</p> <p>One can easily solve this local LQR by performing <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">discrete-time Riccati differential equation</a> and calculating gain matrix \(K_t, k_t\). Then the optimal control \(\delta u_t = k_t + K_t \delta x_t\). Iterating the whole process until convergence.</p> <hr/> <h2 id="4-todo-multipath">4 TODO: MultiPath</h2> <p>The core of MultiPath++ is to parameterize a <strong>multimodal probability distribution</strong> of future trajectories directly from historical trajectories and map information via a deep network.</p> <h3 id="41-mathematical-principles">4.1 Mathematical Principles</h3> <h4 id="trajectory-distribution-modeling-gmmmog">Trajectory Distribution Modeling: GMM/MoG</h4> <p>MultiPath++ predicts a distribution of future behavior parameterized as a Gaussian Mixture Model (<strong>GMM</strong>; ), with a mixture of \(M\) modes:</p> \[p(Y|X) = \sum_{k=1}^K \pi_k \cdot \mathcal{N}(Y; \mu_k(X), \Sigma_k(X))\] <h4 id="loss-function">Loss Function</h4> \[\mathcal{L}_{NLL}= -\log p(Y|X) = -\sum_{m=1}^M \sum_{k=1}^K \mathbb{1}(k = \hat{k}^m) \left[ \log \pi(a^k | x^m; \theta) + \sum_{t=1}^T \log \mathcal{N}(s_t^k | a_t^k + \mu_t^k, \Sigma_t^k; x^m; \theta) \right].\] <h4 id="ensemble-models-multipath">Ensemble models: Multipath++</h4> <p>E ensemble models, for each with L modes &gt; M (with redudancy)</p> <p>Select M modes from M’ = EL predicted heads:</p> <ul> <li>select M cluster centroids from in a greedy fashion</li> <li>iteratively update the parameters of M centraoids</li> </ul> <p><a href="https://zhuanlan.zhihu.com/p/1892876738373060311">zhihu:</a></p> <p>TODO: exploding gradients…</p> <h2 id="references">References</h2> <ul> <li>MultiPath++ (Waymo)</li> <li>Kalman, <em>Linear Filtering and Prediction</em></li> <li>Li &amp; Todorov, <em>iLQR</em></li> </ul>]]></content><author><name>Yuan Zhang</name></author><category term="Blog"/><summary type="html"><![CDATA[Date: 2026-01-31 Estimated Reading Time: 30 min Author: Yuan Zhang]]></summary></entry><entry><title type="html">Hello World</title><link href="https://mikezhang95.github.io/blog/2018/05/10/helloWorld/" rel="alternate" type="text/html" title="Hello World"/><published>2018-05-10T00:00:00+00:00</published><updated>2018-05-10T00:00:00+00:00</updated><id>https://mikezhang95.github.io/blog/2018/05/10/helloWorld</id><content type="html" xml:base="https://mikezhang95.github.io/blog/2018/05/10/helloWorld/"><![CDATA[<p>This is my first personal page.</p> <p>Welcome.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Yuan</name></author><category term="Blog"/><summary type="html"><![CDATA[This is my new page.]]></summary></entry></feed>