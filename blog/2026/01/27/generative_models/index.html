<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Generative Models | Yuan Zhang</title> <meta name="author" content="Yuan Zhang"> <meta name="description" content=" This is the personal webpage of Yuan. A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mikezhang95.github.io/blog/2026/01/27/generative_models/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            // 指定行内公式的识别符
            inlineMath: [['$','$'], ['\\(','\\)']],
            // 避免被当做 HTML 标签处理
            processEscapes: true
        }
    });
    </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <style>table{border-collapse:collapse;width:100%}th,td{border:1px solid #ddd;padding:8px}th{background-color:#f2f2f2}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Yuan Zhang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generative Models</h1> <p class="post-meta">January 27, 2026• Yuan</p> <p class="post-tags"> <a href="/blog/2026"> <i class="fas fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/blog"> <i class="fas fa-hashtag fa-sm"></i> Blog</a>   </p> </header> <article class="post-content"> <p>Generative modeling is a core building block across modern machine learning, from image synthesis to trajectory generation and world modeling.</p> <p>This post reviews two dominant paradigms: generative adversarial networks (GANs) and diffusion models, and their variants.</p> <hr> <h2 id="1-what-is-a-generative-model">1. What Is a Generative Model?</h2> <p>Given data \(x \sim p_\text{data}(x)\), a generative model learns an approximation \(p_\theta(x)\) such that:</p> <ul> <li>generates samples that resemble real data</li> <li>covers the full data distribution.</li> </ul> <p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt="generative-overview" style="zoom: 50%;"></p> <hr> <h2 id="2-todo-variational-auto-encoder-vae">2. TODO: Variational Auto-Encoder (VAE)</h2> <hr> <h2 id="3-generative-adversarial-networks-gans-1">3. Generative Adversarial Networks (GANs) [1]</h2> <h3 id="31-core-idea">3.1 Core Idea</h3> <p>GANs train two networks:</p> <ul> <li>A generator \(G\) generates synthetic samples (\(x = G(z)\)), given a noise variable input \(z\) who usually follows a standard normal distribution and introducespotential output diversity. It is trained to trick the discriminator to offer a high probability.</li> <li>Discriminator \(D\) outputs the <strong>probability</strong> of a given sample coming from the real dataset (\(D(x)\)). It is trained to distinguish the fake samples from the real ones.</li> </ul> <p><img src="https://lilianweng.github.io/posts/2017-08-20-gan/GAN.png" alt="GAN" style="zoom:50%;"></p> <p>These two models compete against each other during the training process: the generator \(G\) is trying hard to trick the discriminator, while the critic model \(D\) is trying hard not to be cheated, which forms a zero-sum game.</p> <p>Suppose \(p_r,p_g,p_z\) distributions over the real sample \(x\), generated sample \(x\) and random noise \(z\) (usually a standard normal distribution). The minimax objective is like:</p> \[\begin{aligned} \min_G \max_D L(G, D) = \mathbb{E}_{x \sim p_r}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))] \\ = \mathbb{E}_{x \sim p_r}[\log D(x)] + \mathbb{E}_{x \sim p_g}[\log(1 - D(x)]. \end{aligned}\] <h3 id="32-what-is-the-optimal-value-for-d">3.2 What is the optimal value for \(D\)?</h3> <p>Examinte the best value for \(D\) by calulating the stationary point:</p> \[D^*(x) = \frac{p_r(x)}{p_r(x) + p_g(x)}.\] <h3 id="33-what-does-the-loss-function-represent">3.3 What does the loss function represent?</h3> <p>Given the optimal discriminator \(D^*\), find the relations between real and synthetic data distributions \(p_r\) and \(p_g\):</p> \[\begin{aligned} L(G, D^*) = \mathbb{E}_{x \sim p_r}[\log \frac{p_r(x)}{p_r(x)+p_g(x)}] + \mathbb{E}_{x \sim p_g}[\log \frac{p_g(x)}{p_r(x)+p_g(x)}] \\ = D_{KL}(p_r\|\frac{p_r+p_g}{2}) - \log2 + D_{KL}(p_g\|\frac{p_r+p_g}{2}) - \log2 \\ = 2D_{JS}(p_r \| p_g) - \log4. \end{aligned}\] <h3 id="34-todo-problems-in-gans">3.4 TODO: Problems in GANs</h3> <ul> <li>Hard to achieve Nash equilibrium</li> <li>Low dimensional supports</li> <li>Vanishing gradient</li> <li>Mode collapse</li> <li>Lack of a proper evaluation metric (unknown likelihood of \(p_g\))</li> </ul> <h3 id="35-wasserstein-gan-wgan-2">3.5 Wasserstein GAN (WGAN) [2]</h3> <p><a href="https://en.wikipedia.org/wiki/Wasserstein_metric" rel="external nofollow noopener" target="_blank">Wasserstein Distance</a> is a measure of the distance between two probability distributions, which can be interpreted as the minimum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution. When dealing with the continuous probability domain, the distance formula becomes:</p> \[W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x_1,x_2) \sim \gamma}[ \|x_1 - x_2\| ], \\\\\] <p>where \(\tau(x_1,x_2)\) states the joint distribution that satisfies the boundary conditions of \(\int_{x_2} \gamma(x_1,x_2)dx_2 = p_r(x_1)\) and \(\int_{x_1} \gamma(x_1,x_2)dx_1 = p_g(x_2)\).</p> <h4 id="todo-why-wasserstein-is-better-than-js-or-kl-divergence">TODO: Why Wasserstein is better than JS or KL divergence?</h4> <p><img src="https://lilianweng.github.io/posts/2017-08-20-gan/wasserstein_simple_example.png" alt="wasserstein_simple_example" style="zoom:50%;"></p> <h4 id="use-wasserstein-distance-as-gan-loss-function">Use Wasserstein distance as GAN loss function</h4> <p>It is intractable to exhaust all the possible joint distributions in \(\Pi(p_r, p_g)\) to compute \(\inf_{\gamma\sim \Pi(p_r, p_g)}\) . Thus the authors proposed a smart transformation of the formula based on the <strong>Kantorovich-Rubinstein duality</strong> to:</p> \[D_W(p_r, p_g) = \frac{1}{K} \sup_{\|f\| \le K} \mathbb{E}_{x\sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g} [f(x)],\] <p>where the real-valued function \(f: \mathbb{R} \to \mathbb{R}\) should be K-Lipschitz continuous. Read this <a href="https://vincentherrmann.github.io/blog/wasserstein/" rel="external nofollow noopener" target="_blank">blog</a> for more about this duality transformation. In the modified Wasserstein GAN, the “discriminator” model is used to find a <strong>good</strong> function \(f_w\) parametrized by \(w \in W\) and the loss function is configured as measuring the Wasserstein distance between \(p_r\) and \(p_g\):</p> \[L(p_r,p_g) = D_W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x\sim p_r}[f_w(x)] - \mathbb{E}_{x \sim p_g} [f_w(x)].\] <p>To maintain the K-Lipschitz continuity of function \(f_w\) during training, the paper presents a simple but very practical trick: After every gradient update, clamp the weights \(w\) to a small window, such as \([-0.01, 0.01]\).</p> <h3 id="36-connections-to-actor-critic-methods-3">3.6 Connections to Actor-Critic Methods [3]</h3> <h4 id="the-mathematical-bridge-bilevel-optimization">The Mathematical Bridge: Bilevel Optimization</h4> \[\begin{aligned} x^* = \arg \min_x F(x, y^*(x)) \quad\text{(Outer Opt.)} \\ y^*(x) = \arg \min_y f(x, y) \quad\text{(Inner Opt.)}. \end{aligned}\] <table> <thead> <tr> <th style="text-align: left">Component</th> <th style="text-align: left">GANs</th> <th style="text-align: left">Actor-Critic Methods</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Outer Model (x)</strong></td> <td style="text-align: left">Discriminator (\(D\))</td> <td style="text-align: left">Critic (\(Q\)-function)</td> </tr> <tr> <td style="text-align: left"><strong>Outer Objective (F)</strong></td> <td style="text-align: left">\(-L(G, D)\)</td> <td style="text-align: left">\(-\mathbb{E}_{s,a}[D_{KL}(\mathbb{E}_{r,s',a'}[r+\gamma Q(s', a')]|Q (s, a)]\)</td> </tr> <tr> <td style="text-align: left"><strong>Inner Model (y)</strong></td> <td style="text-align: left">Generator (\(G\))</td> <td style="text-align: left">Actor (Policy \(\pi\))</td> </tr> <tr> <td style="text-align: left"><strong>Inner Objective (f)</strong></td> <td style="text-align: left">\(-\mathbb{E}_{z\sim p_z}[\log D(G(z))]\)</td> <td style="text-align: left">\(-\mathbb{E}_{s \sim \mu,a\sim\pi}[Q(s, a)]\)</td> </tr> </tbody> </table> <p>​</p> <h4 id="gans-as-a-kind-of-actor-critic">GANs as a kind of Actor-Critic</h4> <p>For an RL environment with:</p> <ul> <li>State: stateless</li> <li>Action: generates an entire image</li> <li>Environment step: randomly decides to show either a real image of a generated image</li> <li>Reward: +1 if a real image is shown, 0 if the generated image is shown</li> </ul> <p>reward/state does not depend on action.</p> <hr> <h2 id="4-diffusion-models">4. Diffusion Models</h2> <p>Several diffusion-based generative models have been proposed with similar ideas underneath, including <em>diffusion probabilistic models</em> (<strong>DPM</strong>; <a href="https://arxiv.org/abs/1503.03585" rel="external nofollow noopener" target="_blank">Sohl-Dickstein et al., 2015</a>), <em>noise-conditioned score network</em> (<strong>NCSN</strong>; <a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">Yang &amp; Ermon, 2019</a>), and <em>denoising diffusion probabilistic models</em> (<strong>DDPM</strong>; <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Ho et al. 2020</a>).</p> <h3 id="41-forward-diffusion-process-from-data-to-noise">4.1 Forward Diffusion Process: From Data to Noise</h3> <p>Given a data point sampled from a real data distribution \(x_0 \sim p_r(x)\), a forward diffusion process, we can gradually add small amount of Gaussian noise in \(T\) steps, producing a sequence os noisy samples \(x_1,\dots,x_T\). The step sizes are controlled by a variance schedule \({\beta_t \in (0,1)}_{t=1}^T\):</p> \[q(x_t|x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I}), q(x_{0:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1}).\] <p>When \(T\to \infty\), \(x_T\) is equivalent to an isotropic Gaussian distribution.</p> <h4 id="reparametrization-tricks-from-x_0-to-x_t">Reparametrization tricks: from \(x_0\) to \(x_t\)</h4> \[\begin{aligned} x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta_t} \epsilon_{t-1} \quad\text{;where $\epsilon_{t-1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$} \\ = \sqrt{(1-\beta_t)(1-\beta_{t-1})}x_{t-2} + \sqrt{1-(1-\beta_t)(1-\beta_{t-1})} \bar{\epsilon}_{t-2} \quad\text{;where $\bar{\epsilon}_{t-2}$ merges two Gaussians} \\ = \dots \\ = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon \quad\text{;where $\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$}\\ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)\mathbf{I}). \end{aligned}\] <h4 id="connections-with-stochasitic-gradient-langevin-dynamics">Connections with stochasitic gradient Langevin dynamics</h4> <p>Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, <em>stochastic gradient Langevin dynamics</em> (<strong>SGLD</strong>; <a href="https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf" rel="external nofollow noopener" target="_blank">Welling &amp; Teh 2011</a>) can produce samples from a probability density \(p(x)\) using only the gradients \(\nabla_x \log p(x)\) in a Markov chain of updates:</p> \[x_t = x_{t-1} + \Delta_t/2 \nabla_x \log p(x_{t-1}) + \sqrt{\Delta_t}\epsilon_t, \quad\text{where $\epsilon_{t} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$},\] <p>where \(\Delta_t\) is the step size. When \(T \to \infty, \epsilon \to 0\), $x_T$ equals to the true probability \(p(x)\). The Gaussian noise avoids collapses into local minima.</p> <h3 id="42-reverse-diffusion-process-from-noise-to-data">4.2 Reverse Diffusion Process: From Noise to Data</h3> <p>If we can reverse the above process and sample from \(p_{\theta}(x_{t-1}\vert x_t)\), we will be able to recreate the true sample from a Gaussian noise input, \(x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\):</p> \[p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};\mu_{\theta}(x_t, t), \Sigma_\theta(x_t, t)), p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t).\] <h4 id="distribution-representation-1-from-x_t-x_0-to-x_t-1">Distribution Representation 1: from \(x_t, x_0\) to \(x_{t-1}\)</h4> <p>It is noteworthy that the reverse conditional probability is tractable when conditioned on real sample \(x_0\) :</p> \[\begin{aligned} q(x_{t-1}|x_t, x_0) = \mathcal{N}(x_{t-1};\tilde{\mu}(x_t, x_0), \tilde{\beta}_t\mathbf{I}), \\ \tilde{\mu}(x_t, x_0) = \frac{\sqrt{1-\beta_t}( 1- \bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t} x_0 \\ \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t. \end{aligned}\] <h4 id="distribution-representation-2-from-x_t-to-x_t-1">Distribution Representation 2: from \(x_t\) to \(x_{t-1}\)</h4> <p>From the reparametrization tricks above, we can replace \(x_0\) with \(x_t\):</p> \[\begin{aligned} \tilde{\mu}(x_t) = \frac{\sqrt{1-\beta_t}( 1- \bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}\frac{1}{\sqrt{\bar{\alpha}_t}} (x_t - \sqrt{1-\bar{\alpha}_t} \epsilon \\ = \frac{1}{\sqrt{1-\beta_t}} (x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon) \quad\text{;where $\bar{\alpha}_{t} = \bar{\alpha}_{t-1}(1-\beta_t)$} \end{aligned}\] <p>The above derivations can be achieved by Bayesian rule, see this <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">blog</a> for details. We want to use a parametrized function \(\mu_{\theta}(x_t, t)\) to represent \(\tilde{\mu}(x_t)\). Since \(x_t\) is known during training, we only need to learn to predict noises \(\epsilon\) with \(\epsilon_\theta(x_t, t)\).</p> <h3 id="43-training-and-sampling-process">4.3 Training and Sampling Process</h3> <p>TODO: dervie VLB loss:</p> <p>TODO: dervie NCSN loss:</p> <p>TODO: dervie DDPM loss:</p> \[\begin{aligned} L_\text{DDPM} = \mathbb{E}_{x_0, t \sim [1, T], \epsilon}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2 ] \\ = \mathbb{E}_{x_0, t \sim [1, T], \epsilon}[\|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\|^2 ] \\ \end{aligned}\] <p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM-algo.png" alt="DDPM-algo" style="zoom: 33%;"></p> <h2 id="5-flow-matching-models">5. Flow-Matching Models</h2> <h3 id="51-mathematical-background">5.1 Mathematical Background</h3> <h4 id="forward-diffusion-as-stochasitic-diffential-equation-sde">Forward Diffusion as Stochasitic Diffential Equation (SDE)</h4> <p>If we make the diffusion step \(\Delta t \to 0\) and the forward diffusion process becomes a stochastic differential equation (SDE):</p> \[dx = f(x,t)dt + g(t)dw\] <p>where:</p> <ul> <li>\(w\) represents a Wiener process (i.e., Brownian random motion);</li> <li>\(f(\cdot, t)\) is called the <strong>drift;</strong> </li> <li>\(g(t)\) is called the <strong>diffusion</strong> term.</li> </ul> <table> <thead> <tr> <th>Diffusion Methods</th> <th>\(f(x, t)\)</th> <th>\(g(t)\)</th> </tr> </thead> <tbody> <tr> <td>DDPM</td> <td>\(-\frac{1}{2}\beta_tx\)</td> <td>\(\sqrt{\beta_t}\)</td> </tr> <tr> <td>NCSN</td> <td>\(0\)</td> <td>\(\sqrt{\frac{d\sigma_t^2}{dt}}\)</td> </tr> </tbody> </table> <p>By carefully designing noise scheduling \(\beta_t\) and \(\sigma_t\) , DDPM and NCSN can generate the same probability path \(\{p_t(x)\}\) and different SDEs are just different mathematical formulas to describe the same process,</p> <h4 id="equivelance-between-sde-and-ordinary-differential-equation-ode">Equivelance between SDE and Ordinary Differential Equation (ODE)</h4> <p><a href="https://arxiv.org/pdf/2011.13456" rel="external nofollow noopener" target="_blank">Song 2021</a> has proved that <em>for any SDE, it has a corresponding ordinary differential equation (ODE) inducing the same \(p_t(x)\)</em>, which is called <strong>probability flow ODE</strong> with the following format: \(\frac{dx}{dt} = f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x)\)</p> <p>This connection transforms a random “walk” to a deterministic “flow” along the ODE trajectory, (1) easing the sampling process of the diffusion process (<a href="https://arxiv.org/abs/2010.02502" rel="external nofollow noopener" target="_blank">DDIM, Song 2021</a>) and (2) inducing following simpler flow-matching methods.</p> <h3 id="52-flow-matching-methods">5.2 Flow-Matching Methods</h3> <p>Probability flow ODE relies on a global score-based function \(\nabla_x \log p_t(x)\) to generate samples, which is difficult to learn. While flow-matching methods directly design a simple path from real data \(x_0\) to pure noise \(x_1\). The simplest path is a constant-speed motion \(x_t=(1-t)x_0 + tx_1\). We need to learn a parametrized function \(v_{\theta}(x_t, t)\) to match the real speed $x_1-x_0$.</p> <h3 id="53-training-and-sampling-process">5.3 Training and Sampling Process</h3> <p><img src="https://raw.githubusercontent.com/mikezhang95/MyImages/main/img/202602021613294.png" alt="Screenshot 2026-02-02 at 16.13.04" style="zoom:80%;"></p> <p>\(z\) and \(x\) can be the same space or \(z\) be the latent space and flow-matching happens on that space.</p> <h2 id="references">References</h2> <p>[1] Goodfellow, Ian, et al. <a href="https://arxiv.org/pdf/1406.2661.pdf" rel="external nofollow noopener" target="_blank">“Generative Adversarial Nets.”</a> Neurips, 2014.</p> <p>[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. <a href="https://arxiv.org/pdf/1701.07875.pdf" rel="external nofollow noopener" target="_blank">“Wasserstein GAN.”</a> ICML, 2017.</p> <p>[3] David Pfau, Oriol Vinyals. <a href="">“Connecting Generative Adversarial Networks and Actor-Critic Methods.”</a> arXiv, 2016.</p> <p>[4] Calvin Luo. <a href="https://arxiv.org/pdf/2208.11970" rel="external nofollow noopener" target="_blank">“Understanding Diffusion Models: A Unified Perspective.”</a> arXiv, 2022.</p> <p>TODO: unify references and hyperlinks in the blog.</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Yuan Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>